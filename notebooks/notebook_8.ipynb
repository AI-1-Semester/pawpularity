{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Step 1: Define the Custom Dataset Class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.annotations.iloc[idx, 0]\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        labels = self.annotations.iloc[idx, 1:].astype('float32').values  # Load all labels\n",
    "        return image, torch.tensor(labels)\n",
    "    \n",
    "# Step 2: Define Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a standard size\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize with mean and std\n",
    "])\n",
    "\n",
    "# Step 3: Initialize the Dataset and DataLoader\n",
    "csv_file = '../data/train/train.csv'  # Path to the CSV file\n",
    "img_dir = '../data/train/train_images'  # Path to the image directory\n",
    "\n",
    "dataset = CustomImageDataset(csv_file, img_dir, transform)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Limit the Dataset size\n",
    "subset_indices = list(range(500))\n",
    "subset_dataset = Subset(dataset, subset_indices)\n",
    "\n",
    "# Define dataloaders\n",
    "train_loader = DataLoader(subset_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(subset_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN Feature Extractor\n",
    "class CustomFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 512)  # Assuming input images are 224x224\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a  Classifier\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer to prevent overfitting\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_labels)  # Modify the number of output labels\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "num_labels = 13  # Number of labels to predict\n",
    "input_dim = 512\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "feature_extractor = CustomFeatureExtractor().to(device)\n",
    "classifier = MultiLabelClassifier(input_dim, num_labels).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Train Loss: 0.6295, Train Acc: 0.3509, Val Loss: 0.6229, Val Acc: 0.3509\n",
      "Epoch [2/25], Train Loss: 0.6171, Train Acc: 0.3509, Val Loss: 0.6104, Val Acc: 0.3509\n",
      "Epoch [3/25], Train Loss: 0.6045, Train Acc: 0.3509, Val Loss: 0.5976, Val Acc: 0.3509\n",
      "Epoch [4/25], Train Loss: 0.5914, Train Acc: 0.3509, Val Loss: 0.5845, Val Acc: 0.3509\n",
      "Epoch [5/25], Train Loss: 0.5780, Train Acc: 0.3509, Val Loss: 0.5706, Val Acc: 0.3509\n",
      "Epoch [6/25], Train Loss: 0.5637, Train Acc: 0.3509, Val Loss: 0.5557, Val Acc: 0.3509\n",
      "Epoch [7/25], Train Loss: 0.5485, Train Acc: 0.3509, Val Loss: 0.5402, Val Acc: 0.3509\n",
      "Epoch [8/25], Train Loss: 0.5323, Train Acc: 0.3509, Val Loss: 0.5233, Val Acc: 0.3509\n",
      "Epoch [9/25], Train Loss: 0.5148, Train Acc: 0.3514, Val Loss: 0.5050, Val Acc: 0.3528\n",
      "Epoch [10/25], Train Loss: 0.4953, Train Acc: 0.3566, Val Loss: 0.4841, Val Acc: 0.3631\n",
      "Epoch [11/25], Train Loss: 0.4739, Train Acc: 0.3765, Val Loss: 0.4621, Val Acc: 0.3952\n",
      "Epoch [12/25], Train Loss: 0.4515, Train Acc: 0.4074, Val Loss: 0.4399, Val Acc: 0.4189\n",
      "Epoch [13/25], Train Loss: 0.4292, Train Acc: 0.4238, Val Loss: 0.4170, Val Acc: 0.4263\n",
      "Epoch [14/25], Train Loss: 0.4065, Train Acc: 0.4268, Val Loss: 0.3947, Val Acc: 0.4269\n",
      "Epoch [15/25], Train Loss: 0.3839, Train Acc: 0.4269, Val Loss: 0.3719, Val Acc: 0.4269\n",
      "Epoch [16/25], Train Loss: 0.3610, Train Acc: 0.4269, Val Loss: 0.3489, Val Acc: 0.4269\n",
      "Epoch [17/25], Train Loss: 0.3376, Train Acc: 0.4272, Val Loss: 0.3249, Val Acc: 0.4274\n",
      "Epoch [18/25], Train Loss: 0.3130, Train Acc: 0.4291, Val Loss: 0.2996, Val Acc: 0.4331\n",
      "Epoch [19/25], Train Loss: 0.2870, Train Acc: 0.4417, Val Loss: 0.2729, Val Acc: 0.4546\n",
      "Epoch [20/25], Train Loss: 0.2596, Train Acc: 0.4683, Val Loss: 0.2447, Val Acc: 0.4798\n",
      "Epoch [21/25], Train Loss: 0.2311, Train Acc: 0.4851, Val Loss: 0.2155, Val Acc: 0.4869\n",
      "Epoch [22/25], Train Loss: 0.2010, Train Acc: 0.4905, Val Loss: 0.1846, Val Acc: 0.4972\n",
      "Epoch [23/25], Train Loss: 0.1695, Train Acc: 0.5105, Val Loss: 0.1524, Val Acc: 0.5243\n",
      "Epoch [24/25], Train Loss: 0.1367, Train Acc: 0.5317, Val Loss: 0.1189, Val Acc: 0.5378\n",
      "Epoch [25/25], Train Loss: 0.1020, Train Acc: 0.5389, Val Loss: 0.0831, Val Acc: 0.5398\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(predictions, labels, threshold=0.5):\n",
    "    preds = (predictions > threshold).float()\n",
    "    correct = (preds == labels).float().sum()\n",
    "    accuracy = correct / (labels.size(0) * labels.size(1))\n",
    "    return accuracy\n",
    "\n",
    "# Training loop with accuracy calculation\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        features = feature_extractor(images)\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        train_accuracy += calculate_accuracy(outputs, labels).item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    classifier.eval()\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            features = feature_extractor(images)\n",
    "            outputs = classifier(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            val_accuracy += calculate_accuracy(outputs, labels).item() * images.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: {'Subject Focus': 0.0, 'Eyes': 0.0, 'Face': 1.0, 'Near': 0.0, 'Action': 0.0, 'Accessory': 1.0, 'Group': 1.0, 'Collage': 0.0, 'Human': 0.0, 'Occlusion': 0.0, 'Info': 0.0, 'Blur': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Prediction on a single image\n",
    "def load_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Single image prediction\n",
    "image_path = '../data/train/train_images/02581632f146cccfcfc93005ef5f907e.jpg'  # Replace with the path to your image\n",
    "image = load_image(image_path, transform).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = feature_extractor(image)\n",
    "    output = classifier(features).squeeze()\n",
    "    predictions = (output > 0.5).float()\n",
    "\n",
    "labels = [\"Subject Focus\", \"Eyes\", \"Face\", \"Near\", \"Action\", \"Accessory\", \"Group\", \"Collage\", \"Human\", \"Occlusion\", \"Info\", \"Blur\"]\n",
    "predictions_dict = {label: predictions[i].item() for i, label in enumerate(labels)}\n",
    "\n",
    "print(\"Predictions:\", predictions_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pawpularityVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
