{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Step 1: Define the Custom Dataset Class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.annotations.iloc[idx, 0]\n",
    "        img_path = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        labels = self.annotations.iloc[idx, 1:].astype('float32').values  # Load all labels\n",
    "        return image, torch.tensor(labels)\n",
    "    \n",
    "# Step 2: Define Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a standard size\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize with mean and std\n",
    "])\n",
    "\n",
    "# Step 3: Initialize the Dataset and DataLoader\n",
    "csv_file = '../data/train/train.csv'  # Path to the CSV file\n",
    "img_dir = '../data/train/train_images'  # Path to the image directory\n",
    "\n",
    "dataset = CustomImageDataset(csv_file, img_dir, transform)\n",
    "\n",
    "# # Limit the Dataset size\n",
    "subset_indices = list(range(1000))\n",
    "subset_dataset = Subset(dataset, subset_indices)\n",
    "\n",
    "# Split dataset into training, validation, and test sets\n",
    "train_size = int(0.8 * len(subset_dataset))\n",
    "val_size = len(subset_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(subset_dataset, [train_size, val_size])\n",
    "\n",
    "# Define relevant variables for the ML task (Hyperparameters)\n",
    "batch_size = 8\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 10\n",
    "\n",
    "# Define dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create custom CNN Feature Extractor\n",
    "class CustomFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # Additional layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 14 * 14, 512)  # Adjust for new feature size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define a Classifier\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer to prevent overfitting\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_labels)  # Modify the number of output labels\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Initialize the Model, Loss, and Optimizer\n",
    "\n",
    "num_labels = 13  # Number of labels to predict\n",
    "input_dim = 512\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "feature_extractor = CustomFeatureExtractor().to(device)\n",
    "classifier = MultiLabelClassifier(input_dim, num_labels).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.8785, Train Acc: 0.6894, Val Loss: 0.8527, Val Acc: 0.7004\n",
      "Epoch [2/10], Train Loss: 0.8340, Train Acc: 0.6894, Val Loss: 0.8086, Val Acc: 0.7004\n",
      "Epoch [3/10], Train Loss: 0.7902, Train Acc: 0.6894, Val Loss: 0.7664, Val Acc: 0.7004\n",
      "Epoch [4/10], Train Loss: 0.7457, Train Acc: 0.6894, Val Loss: 0.7201, Val Acc: 0.7004\n",
      "Epoch [5/10], Train Loss: 0.6955, Train Acc: 0.6894, Val Loss: 0.6678, Val Acc: 0.7004\n",
      "Epoch [6/10], Train Loss: 0.6403, Train Acc: 0.6894, Val Loss: 0.6111, Val Acc: 0.7004\n",
      "Epoch [7/10], Train Loss: 0.5820, Train Acc: 0.6894, Val Loss: 0.5534, Val Acc: 0.7004\n",
      "Epoch [8/10], Train Loss: 0.5232, Train Acc: 0.6917, Val Loss: 0.4941, Val Acc: 0.7342\n",
      "Epoch [9/10], Train Loss: 0.4607, Train Acc: 0.7595, Val Loss: 0.4310, Val Acc: 0.7773\n",
      "Epoch [10/10], Train Loss: 0.3943, Train Acc: 0.7644, Val Loss: 0.3633, Val Acc: 0.7773\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Training Loop with Validation\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(predictions, labels, threshold=0.5):\n",
    "    preds = (predictions > threshold).float()\n",
    "    correct = (preds == labels).float().sum()\n",
    "    accuracy = correct / (labels.size(0) * labels.size(1))\n",
    "    return accuracy\n",
    "\n",
    "# Training loop with accuracy calculation\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        features = feature_extractor(images)\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        train_accuracy += calculate_accuracy(outputs, labels).item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    classifier.eval()\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            features = feature_extractor(images)\n",
    "            outputs = classifier(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            val_accuracy += calculate_accuracy(outputs, labels).item() * images.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(feature_extractor.state_dict(), '../data/train/find_features.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: {'Subject Focus': 0.0, 'Eyes': 1.0, 'Face': 1.0, 'Near': 0.0, 'Action': 0.0, 'Accessory': 0.0, 'Group': 0.0, 'Collage': 0.0, 'Human': 0.0, 'Occlusion': 0.0, 'Info': 0.0, 'Blur': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "def load_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Load the pre-trained weights\n",
    "feature_extractor.load_state_dict(torch.load('../data/train/find_features.pth'))\n",
    "\n",
    "image_path = '../data/train/train_images/0f93d1482d7fb9f2b6a5085ee729142b.jpg'  # Replace with the path to your image\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = load_image(image_path, transform).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = feature_extractor(image)\n",
    "    output = classifier(features).squeeze()\n",
    "    predictions = (output > 0.5).float()\n",
    "\n",
    "labels = [\"Subject Focus\", \"Eyes\", \"Face\", \"Near\", \"Action\", \"Accessory\", \"Group\", \"Collage\", \"Human\", \"Occlusion\", \"Info\", \"Blur\"]\n",
    "predictions_dict = {label: predictions[i].item() for i, label in enumerate(labels)}\n",
    "\n",
    "print(\"Predictions:\", predictions_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pawpularityVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
